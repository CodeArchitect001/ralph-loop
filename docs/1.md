设计成一个 "迷你版 V6.0 数据处理管道"——用 Python 模拟你熟悉的 Flink 流计算逻辑，但简单到明天一天能跑完。

---
项目背景（模拟真实场景）
你有一台服务器在疯狂打印日志，每行是一个 JSON：
{"timestamp": "2025-01-15T10:23:45", "level": "ERROR", "service": "payment", "latency_ms": 1250, "msg": "timeout"}
{"timestamp": "2025-01-15T10:23:46", "level": "INFO", "service": "user", "latency_ms": 45, "msg": "login"}
...
但问题是：
- 有些行是损坏的 JSON（缺括号、编码错误）
- 需要实时统计：错误率、P99 延迟、服务健康度
- 最后生成一个 HTML 监控面板

---
项目结构（提前准备）
mkdir ~/ralph-log-analyzer && cd ~/ralph-log-analyzer
git init

mkdir -p src tests/data .ralph
touch src/__init__.py src/parser.py src/analyzer.py src/reporter.py src/cli.py 
touch tests/test_parser.py tests/test_analyzer.py 
touch .ralph/stories.json .ralph/progress.txt PROMPT.md 

---
1. 测试数据（tests/data/raw_logs.jsonl）
复制 20 行混合数据（故意弄些脏数据）：
{"timestamp": "2025-01-15T10:23:45", "level": "ERROR", "service": "payment", "latency_ms": 1250, "msg": "timeout"}
{"timestamp": "2025-01-15T10:23:45", "level": "INFO", "service": "auth", "latency_ms": 23, "msg": "token_valid"}
{"timestamp": "2025-01-15T10:23:46", "level": "WARN", "service": "payment", "latency_ms": 890, "msg": "slow_query"
{"timestamp": "2025-01-15T10:23:46", "level": "ERROR", "service": "db", "latency_ms": 5000, "msg": "connection_lost"}
{"timestamp": "2025-01-15T10:23:47", "level": "INFO", "service": "user", "latency_ms": 45, "msg": "login"}
{"timestamp": "2025-01-15T10:23:47", "level": "INFO", "service": "payment", "latency_ms": 120, "msg": "processed"}
{"timestamp": "2025-01-15T10:23:48", "level": "ERROR", "service": "auth", "latency_ms": 3000, "msg": "rate_limited"}
{"timestamp": "broken json here
{"timestamp": "2025-01-15T10:23:49", "level": "INFO", "service": "user", "latency_ms": 30, "msg": "logout"}
{"timestamp": "2025-01-15T10:23:49", "level": "WARN", "service": "cache", "latency_ms": 600, "msg": "miss"}
{"timestamp": "2025-01-15T10:23:50", "level": "ERROR", "service": "payment", "latency_ms": 2100, "msg": "declined"}
{"timestamp": "2025-01-15T10:23:50", "level": "INFO", "service": "auth", "latency_ms": 15, "msg": "check"}
{"timestamp": "2025-01-15T10:23:51", "level": "INFO", "service": "db", "latency_ms": 80, "msg": "select"}
{"timestamp": "2025-01-15T10:23:51", "level": "ERROR", "service": "user", "latency_ms": 4500, "msg": "timeout"}
{"timestamp": "2025-01-15T10:23:52", "level": "INFO", "service": "payment", "latency_ms": 200, "msg": "refund"}
{"timestamp": "2025-01-15T10:23:52", "level": "WARN", "service": "auth", "latency_ms": 750, "msg": "retry"}
{"timestamp": "2025-01-15T10:23:53", "level": "INFO", "service": "cache", "latency_ms": 5, "msg": "hit"}
{"timestamp": "2025-01-15T10:23:53", "level": "ERROR", "service": "db", "latency_ms": 3200, "msg": "deadlock"}
{"timestamp": "2025-01-15T10:23:54", "level": "INFO", "service": "user", "latency_ms": 60, "msg": "view"}
{"timestamp": "2025-01-15T10:23:54", "level": "INFO", "service": "payment", "latency_ms": 110, "msg": "webhook"}
{"timestamp": "2025-01-15T10:23:55", "level": "ERROR", "service": "auth", "latency_ms": 2800, "msg": "invalid_key"}

---
2. 任务清单（.ralph/stories.json）
{
  "project": "Log Stream Analyzer",
  "language": "Python 3.9+",
  "stories": [
    {
      "id": 1,
      "task": "实现鲁棒的 JSONL 解析器（src/parser.py）",
      "description": "读取 .jsonl 文件，跳过损坏的 JSON 行，返回结构化数据",
      "acceptance_criteria": [
        "能解析 tests/data/raw_logs.jsonl 并跳过损坏行（第3行和第8行）",
        "返回 List[Dict]，每个 Dict 包含 timestamp, level, service, latency_ms, msg",
        "提供 parse_line() 单条解析函数和 parse_file() 批量解析函数",
        "损坏行记录到 stderr，但不中断处理"
      ],
      "test_command": "python -m pytest tests/test_parser.py -v",
      "passes": false
    },
    {
      "id": 2,
      "task": "实现流式分析引擎（src/analyzer.py）",
      "description": "计算统计指标：错误率、各服务 P99 延迟、日志总数",
      "acceptance_criteria": [
        "实现 LogAnalyzer 类，支持 add_record() 逐条处理和 get_stats() 获取结果",
        "统计指标包括：total_logs, error_count, error_rate%, 各服务的 p50/p99 延迟",
        "使用 heapq 或 sorted 计算百分位数，不准用 numpy（保持轻量）",
        "处理空输入不报错"
      ],
      "test_command": "python -m pytest tests/test_analyzer.py -v",
      "passes": false
    },
    {
      "id": 3,
      "task": "实现 HTML 报告生成器（src/reporter.py）",
      "description": "生成漂亮的 HTML 监控面板，包含图表（用纯 HTML/CSS，无需 JS 库）",
      "acceptance_criteria": [
        "生成包含以下内容的 HTML：标题、统计摘要卡片（总数/错误率/P99）、服务详情表格",
        "错误率用颜色标识：<1% 绿色，1-5% 黄色，>5% 红色",
        "HTML 文件可独立打开，不依赖外部 CDN（内联样式）",
        "输出路径通过参数指定，默认 report.html"
      ],
      "test_command": "python -c \"from src.reporter import generate_report; generate_report({'total': 100}, 'test.html')\" && test -f test.html",
      "passes": false
    },
    {
      "id": 4,
      "task": "实现 CLI 接口（src/cli.py）",
      "description": "命令行入口，使用 argparse 处理参数",
      "acceptance_criteria": [
        "命令行接口：python -m src.cli --input logs.jsonl --output report.html",
        "支持 --window-size 参数（只分析最近 N 条日志，默认全部）",
        "支持 --verbose 标志，显示处理进度",
        "返回码：成功 0，文件不存在 1，解析错误 2"
      ],
      "test_command": "python -m src.cli --input tests/data/raw_logs.jsonl --output /tmp/test_report.html && test -f /tmp/test_report.html",
      "passes": false
    },
    {
      "id": 5,
      "task": "端到端集成与测试",
      "description": "完整流程验证和代码清理",
      "acceptance_criteria": [
        "运行完整流程：python -m src.cli --input tests/data/raw_logs.jsonl --output final_report.html",
        "生成的 final_report.html 能用浏览器正常打开，包含所有统计信息",
        "所有 pytest 通过，无 pylint 警告（pylint src/ 评分 > 8.0）",
        "README.md 包含使用说明和示例输出截图"
      ],
      "test_command": "pylint src/ --fail-under=8.0 && pytest tests/ -v",
      "passes": false
    }
  ]
}

---
3. PROMPT.md（通用模板）
# Ralph Loop - Log Stream Analyzer 开发指令

你是专业的 Python 工程师，正在开发日志流分析器。

## 工作流程（必须严格遵循）

1. **读取状态**：读取 @.ralph/stories.json 和 @.ralph/progress.txt 了解当前进度
2. **选择任务**：找出第一个 `passes: false` 的 story，按 id 顺序执行
3. **单任务聚焦**：一次只处理一个 story，不要提前实现后续功能
4. **开发循环**：
   - 创建/修改代码文件
   - 运行该 story 指定的 test_command
   - 如果失败，分析错误并修复，重新测试
   - 如果通过，进行下一步
5. **更新状态**：
   - 将该 story 的 `passes` 改为 `true`
   - 在 progress.txt 追加：`[timestamp] Story X completed: [关键实现细节，遇到什么问题，如何解决]`
   - git commit -m "feat: complete story X - [简述]"
6. **结束判断**：
   - 如果还有 `passes: false` 的 story，继续下一轮
   - 如果全部完成，输出：<promise>COMPLETE</promise>

## 代码规范

- Python 3.9+ 语法，类型注解可选但推荐
- 异常处理必须用 try-except，不能裸奔
- 每个函数docstring说明功能
- 不准用外部依赖（标准库 only：json, sys, argparse, html, datetime, heapq, collections）

## 调试信息

如果测试失败，在 progress.txt 中记录：
- 错误类型和堆栈关键行
- 你的修复思路
- 下次迭代要注意什么

## 完成标记

全部 stories 完成后必须输出：
<promise>COMPLETE</promise>

---
4. 明天对比实验方案
上午：Bash 版（方法 A）
cd ~/ralph-log-analyzer

# 清理状态（确保公平）
rm -rf .ralph/progress.txt src/*.py tests/*.py *.html
git checkout -- . || true

# 启动 Bash 循环（透明模式，观察学习）
counter=0
max=25
while [ $counter -lt $max ]; do
  echo "[$(date '+%H:%M:%S')] === Iteration $((counter+1))/$max ==="
  cat PROMPT.md | claude code
  ((counter++))
  echo "[$(date '+%H:%M:%S')] Sleeping 15s..."
  sleep 15
done

# 记录：完成时间、实际迭代次数、最终代码行数
date >> experiment_a.log
观察重点：
- Ralph 第几轮开始理解任务结构？
- 它如何处理损坏的 JSON 行（是跳过还是崩溃）？
- 有没有陷入"修复 A 导致 B 坏"的循环？
下午：官方插件版（方法 B）
# 清理状态（重新开始）
rm -rf .ralph/progress.txt src/*.py tests/*.py *.html
# 恢复 stories.json（被删的话从 git 恢复）
git checkout .ralph/stories.json

# 启动官方插件（安全模式）
/ralph-loop:ralph-loop "开发 Log Stream Analyzer" \
  --completion-promise "COMPLETE" \
  --max-iterations 25 \
  --no-auto-approve

# 记录：完成时间、实际迭代次数、人工干预次数（点了几次 approve）
date >> experiment_b.log
对比重点：
- 完成速度哪个快？
- 代码质量差异（可读性、错误处理完善度）
- 你的体验：哪个更省心？哪个更可控？

---
5. 成功标准（明天晚上验收）
无论用哪种方式，最终要产出：
1. 可运行的 CLI 工具：
python -m src.cli --input tests/data/raw_logs.jsonl --output report.html
# 生成一个能打开的 HTML 报告
2. 测试全部通过：
pytest tests/ -v  # 绿灯
3. 代码质量：
pylint src/ --fail-under=8.0
4. 对比报告（你自己写的笔记，Ralph 不帮你写这个）：
  - Bash 版用了 X 轮，官方版用了 Y 轮
  - Bash 版哪里卡住了？官方版的 --no-auto-approve 是否频繁打断流程？
  - 结论：未来 V6.0 开发选哪个？

---
今晚准备清单
现在（放假第一天晚上）：
1. 在你的虚拟机里创建 ~/ralph-log-analyzer 目录
2. 复制上面的 stories.json 和 PROMPT.md
3. 创建 tests/data/raw_logs.jsonl（那 20 行日志）
4. 确保 claude code 能正常登录（跑 claude code --version 验证）
5. 早睡觉，明天需要专注观察 Ralph 的行为模式
明天上午 9:00：启动 Bash 版，开始实验。
这个项目模拟了 V6.0 的核心挑战（流式数据解析、错误恢复、实时统计），但规模小到一天能完成。跑完这个，你就知道怎么把 Ralph 用在 Neural Sovereign 的 Sidecar 旁路上了。